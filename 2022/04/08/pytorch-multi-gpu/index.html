<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
<meta name="google-site-verification" content="nRdQ64asSdtMIPKmtAL6qJO9Ka3rBCML55licirmHbo" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/panda-apple.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/panda-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/panda-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"timeoverflow.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这里只记录使用 torch.nn.parallel.DistributedDataParallel 进行多卡训练，使用 torch.nn.&#96;&#96;DataParallel 的方法已经不被 PyTorch 官方推荐了，因此不做赘述。">
<meta property="og:type" content="article">
<meta property="og:title" content="使用 DDP 多卡训练">
<meta property="og:url" content="https://timeoverflow.github.io/2022/04/08/pytorch-multi-gpu/index.html">
<meta property="og:site_name" content="时间溢出的地方">
<meta property="og:description" content="这里只记录使用 torch.nn.parallel.DistributedDataParallel 进行多卡训练，使用 torch.nn.&#96;&#96;DataParallel 的方法已经不被 PyTorch 官方推荐了，因此不做赘述。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/Zhao%20Zhang/Desktop/20211209210301.png">
<meta property="article:published_time" content="2022-04-08T05:03:32.000Z">
<meta property="article:modified_time" content="2022-04-08T05:15:09.429Z">
<meta property="article:author" content="TimeOverflow">
<meta property="article:tag" content="Pytorch 多卡训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/Zhao%20Zhang/Desktop/20211209210301.png">

<link rel="canonical" href="https://timeoverflow.github.io/2022/04/08/pytorch-multi-gpu/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>使用 DDP 多卡训练 | 时间溢出的地方</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">时间溢出的地方</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/TimeOverflow" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://timeoverflow.github.io/2022/04/08/pytorch-multi-gpu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="TimeOverflow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="时间溢出的地方">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用 DDP 多卡训练
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-08 13:03:32 / 修改时间：13:15:09" itemprop="dateCreated datePublished" datetime="2022-04-08T13:03:32+08:00">2022-04-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          
            <span id="/2022/04/08/pytorch-multi-gpu/" class="post-meta-item leancloud_visitors" data-flag-title="使用 DDP 多卡训练" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/04/08/pytorch-multi-gpu/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/04/08/pytorch-multi-gpu/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这里只记录使用 <code>torch.nn.parallel.DistributedDataParallel</code> 进行多卡训练，使用 <code>torch.nn.``DataParallel</code> 的方法已经不被 PyTorch 官方推荐了，因此不做赘述。</p>
<span id="more"></span>
<h2 id="原理"><span class="post-title-index">1. </span><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>DDP加速的原理是通过启动多个<strong>进程</strong>，提高同时训练的 <strong>batch size</strong> 来增加并行度的，每一个进程都会加载一个模型，用<strong>不同的数据</strong>进行训练之后得到各自的梯度，然后通过 <strong>Ring-Reduce</strong> 算法获得所有进程的梯度，然后进行相同的梯度下降。注意在训练前和训练后，所有进程的模型参数都是同步了的。</p>
<p>Ring-Reduce 是很简单理解的一个算法：每个进程都从左手边获得一份梯度，然后从右手发送一份梯度（一份指的是一个 GPU 得出的梯度），经过 n 次迭代之后，所有进程都获得了相同的完整的梯度。如下图，假设梯度 $\nabla w$ 是 GPU0 算出来的，第一次 $\nabla w$ 被发送到 GPU1，第二次被 GPU1 发送到 GPU2，第三次被 GPU2 发送到 GPU3，第四次被发送到 GPU4，第五次被发回给 GPU0。这样每个进程都只需要接收一个，发送一个，而且能够清楚知道什么时候结束。</p>
<p><img src="C:/Users/Zhao Zhang/Desktop/20211209210301.png" alt="20211209210301" style="zoom:50%;"></p>
<p>原理部分来源于 <a target="_blank" rel="noopener" href="https://blog.kamino.link/2021/12/09/Pytorch%20DDP%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9/#">这个博客</a></p>
<h2 id="基本概念"><span class="post-title-index">2. </span><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><code>world_size</code>：全局并行的进程数，一般一个 GPU 上一个进程，所以可以理解为使用的所有 GPU 数目。单机多卡时，<code>world_size</code> 就是这台机器上的 GPU 数目；多机多卡时，<code>world_size</code> 就是机器数×每台机器上的 GPU 数目。可以通过 <code>torch.distributed.get_world_size()</code> 获得 <code>world_size</code></li>
<li><code>rank</code>：当前进程在全局的序号，全局指的是所有机器上的进程，<code>rank=0</code> 的进程是 matser 进程，可以通过 <code>torch.distributed.get_rank()</code> 获得当前进程的 <code>rank</code></li>
<li><code>local_rank</code>：当前进程在这台机器上的序号，单机多卡时，<code>local_rank</code> 与 <code>rank</code> 其实是一样的</li>
</ul>
<h2 id="使用流程"><span class="post-title-index">3. </span><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h2><p>首先给出启动方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=n_gpus train.py</span><br></pre></td></tr></table></figure>
<h3 id="准备工作"><span class="post-title-index">3.1. </span><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataloader</span><br></pre></td></tr></table></figure>
<p>使用 <code>torch.distributed.launch</code> 启动时，将会为当前主机创建 <code>nproc_per_node</code> 个进程，每个进程独立执行训练脚本。同时，它还会为每个进程分配一个 <code>local_rank</code> 参数，表示当前进程在当前主机上的编号，需要在 <code>argparse</code> 中加上 <code>--local_rank</code> 来接收这个参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个 local_rank 不用管，由 torch.distributed_launch 来分配</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果在命令行启动时使用这样的命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=n_gpus --use_env train.py</span><br></pre></td></tr></table></figure>
<p>那么当前进程的 <code>local_rank</code> 不会传递给 <code>args.local_rank</code>，而是要通过 <code>os.environ[&#39;LOCAL_RANK&#39;]</code> 获得</p>
</blockquote>
<p>接下来指定当前使用哪块 GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">local_rank = args.local_rank</span><br><span class="line"><span class="comment"># 有的地方说可以使用 local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])</span></span><br><span class="line">torch.cuda.set_device(local_rank)</span><br></pre></td></tr></table></figure>
<p>下面进行 DDP 的初始化，用 <code>torch.distributed.init_process_group</code></p>
<blockquote>
<p><code>torch.distributed.init_process_group</code> 的函数签名为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(backend, </span><br><span class="line">                                     init_method=<span class="literal">None</span>, </span><br><span class="line">                                     timeout=datetime.timedelta(seconds=<span class="number">1800</span>), </span><br><span class="line">                                     world_size=- <span class="number">1</span>, </span><br><span class="line">                                     rank=- <span class="number">1</span>, </span><br><span class="line">                                     store=<span class="literal">None</span>, </span><br><span class="line">                                     group_name=<span class="string">&#x27;&#x27;</span>, </span><br><span class="line">                                     pg_options=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>我们比较关注的参数是 <code>bakcend</code>, <code>init_method</code>, <code>world_size</code>, <code>rank</code></p>
<ul>
<li><p><code>backend</code>：使用什么通信后端，有 <code>mpi</code>, <code>gloo</code> 和 <code>nccl</code> 可选，在 Nvidia 显卡中一般使用 <code>nccl</code></p>
</li>
<li><p><code>init_method</code>：当前进程组的初始化方式，这个不太明白是干啥的，在默认情况下（<code>init_method=None</code> and <code>store=None</code>），<code>init_method</code> 会被设置为 <code>&quot;env://&quot;</code>，表示使用读取环境变量的方式进行初始化，单机多卡时这样使用应该是没问题的，即不指定这个参数。也可以手动指定为 <code>init_method=&quot;env://&quot;</code>。这样默认时，这个进程会自动从本机的环境变量中读取如下数据：</p>
<ul>
<li><p><code>MASTER_PORT</code>：rank0 机器上的一个空闲端口</p>
</li>
<li><p><code>MASTER_ADDR</code>：rank0 机器的地址</p>
</li>
<li><p><code>WORLD_SIZE</code>：这里可以指定，在 <code>init</code> 函数中也可以指定</p>
</li>
<li><p><code>RANK</code>：本机的 <code>rank</code>，也可以在 <code>init</code> 函数中指定</p>
</li>
</ul>
</li>
<li><p><code>world_size</code>：就是全局进程数，在 <code>store=None</code> 的时候是可以不设定的，我看到的很多代码里都没有设置。当然也可以通过 <code>args</code> 传参来给定，或者设置为 <code>int(os.environ[&#39;WORLD_SIZE&#39;])</code></p>
</li>
<li><p><code>rank</code>：当前进程的全局序号，这个在 <code>store=None</code> 的时候也可以不设定，并且很多代码里都没设定。当然，也可以设置为 <code>int(os.environ[&#39;RANK&#39;])</code>，在单机多卡时，应该也可以设置为 <code>local_rank</code></p>
</li>
</ul>
</blockquote>
<p>综上所述，在单机多卡训练时，直接用下面的方式初始化是没什么问题的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dist is torch.distributed</span></span><br><span class="line">dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><code>init_process_group</code> (可通过 <code>dist.is_initialized()</code> 判断) 之後可以用 <code>dist.get_world_size()</code> <code>dist.get_rank()</code> 等方法獲得需要信息，不需要依靠 <code>args</code> 了。</p>
<p><strong>我对 <code>init_method</code> 的理解</strong></p>
<p><code>init_method</code> 一般有 <code>env://</code> 和 <code>tcp://</code> 两种选择。</p>
<ol>
<li><p>对于 <code>env://</code>，一般直接使用 <code>dist.init_process_group(backend=&quot;nccl&quot;, init_method=&quot;env://&quot;)</code> 就可以了（甚至 <code>init_method</code> 也不用显示给出），<code>world_size</code> 和 <code>rank</code> 两个参数可以不给出（至少在单机多卡时是可以的）。在多级多卡的情况下是否要给出，我还不太清楚，给出的话应该也可以通过命令行来传输的，这部分可以参考 MoCo 的代码（MoCo 的代码中，通过命令行给出机器数和当前机器的编号，再转换成 <code>world_size</code> 和 <code>rank</code> 给 <code>init_process_group</code> 方法）。</p>
<p> 单机多卡运行：</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=n_gpus train.py</span><br></pre></td></tr></table></figure>
<p> 多机多卡运行：</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">node 1</span></span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE </span><br><span class="line">       --nnodes=2 --node_rank=0 --master_addr=&quot;192.168.1.1&quot;</span><br><span class="line">       --master_port=1234 train.py</span><br><span class="line"><span class="meta"># </span><span class="language-bash">node 2</span></span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE </span><br><span class="line">       --nnodes=2 --node_rank=1 --master_addr=&quot;192.168.1.1&quot;</span><br><span class="line">       --master_port=1234 train.py</span><br></pre></td></tr></table></figure>
<p> 多机多卡时，需要额外指定 <code>--nnodes</code>, <code>--node_rank</code>, <code>--master_addr</code>, <code>--master_port</code> 参数，这几个参数和 <code>--nproc_per_node</code> 都由 <code>torch.distributed.launch</code> 来处理，在代码中不需要用 <code>args</code> 来接收。</p>
</li>
<li><p>对于 <code>tcp://</code>，需要指定主进程的 IP 和端口号，使用 <code>dist.init_process_group(backend=&quot;nccl&quot;, init_method=&quot;tcp://192.168.1.1:1234&quot;)</code> 这样的设置，<code>world_size</code> 和 <code>rank</code> 需不需要设置不太清楚，但我看到的代码很多都设置了（比如 MoCo）。</p>
<p> 用这种方式初始化的时候，不需要在脚本中用 <code>torch.distributed_launch</code> 来启动。但要指定<strong>机器</strong>数量和当前<strong>机器</strong>的编号，然后转换成 <code>world_size</code> 和 <code>rank</code> 传递给 <code>init_process_group</code>。可以参见 MoCo 的代码</p>
</li>
</ol>
<h3 id="包裹模型"><span class="post-title-index">3.2. </span><a href="#包裹模型" class="headerlink" title="包裹模型"></a>包裹模型</h3><p>在包裹模型前，首先要把模型放到单块 GPU 上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:&#123;&#125;&quot;</span>, local_rank)</span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(device)</span><br><span class="line"></span><br><span class="line">model = DDP(model, device_ids=[local_rank], output_device=local_rank)</span><br></pre></td></tr></table></figure>
<h3 id="数据并行"><span class="post-title-index">3.3. </span><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>DDP 同时起了很多个进程，但是他们用的是同一份数据，我们当然希望每个 GPU 上使用不同的数据，这时需要一个 sampler 自动为每个进程分配不同的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from torch.utils.data.distributed import DistributedSampler</span></span><br><span class="line"><span class="comment"># from torch.utils.data import Dataloader</span></span><br><span class="line"></span><br><span class="line">train_dataset = MyDataset()</span><br><span class="line"><span class="comment"># shuffle 不在 dataloader 中设置，而是在 sampler 中设置</span></span><br><span class="line">train_sampler = DistributedSampler(train_dataset, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># batch_size 指的是一张卡上的 batch_size，总 batch_size 应该是要乘 world_size</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, </span><br><span class="line">                                           batch_size=<span class="number">64</span>, </span><br><span class="line">                                           sampler=train_sampler)</span><br></pre></td></tr></table></figure>
<p><code>DistributedSampler</code> 还有一个参数是 <code>num_replicas</code>，默认是 <code>world_size</code>，所以一般也不用手动设置</p>
<h3 id="训练"><span class="post-title-index">3.4. </span><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>我更愿意把一个 epoch 的训练过程包装成一个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_on_epoch</span>(<span class="params">model, train_loader, criterion, optimizer</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># device 是当前进程的 GPU</span></span><br><span class="line">        data = data.cuda(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">        label = label.cuda(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        pred = model(data)</span><br><span class="line">        loss = criterion(pred, label)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这里的 loss 是当前进程这个 batch 的 loss，要得到所有进程的平均 loss，</span></span><br><span class="line">        <span class="comment"># 需要累加求平均。这一步只是为了输出或者记录 loss 值</span></span><br><span class="line">        dist.all_reduce(loss, op=dist.ReduceOp.SUM)</span><br><span class="line">        loss /= dist.get_world_size()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 只在主进程中打印：</span></span><br><span class="line">        <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;loss of this batch: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss.item()))</span><br></pre></td></tr></table></figure>
<p>现在是训练用的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    train_sampler.set_epoch(epoch)</span><br><span class="line">    train_on_epoch(model, train_loader, criterion, optimizer)</span><br></pre></td></tr></table></figure>
<p>每个 epoch 开始之前，要先用 <code>train_sampler.set_epoch(epoch)</code> 随机 shuffle 一下数据，要不然每个 epoch 里给 GPU 分配的数据都是一样的。</p>
<h3 id="保存模型"><span class="post-title-index">3.5. </span><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><p>只在主进程中保存模型，并且保存的是 <code>model.module</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">    torch.save(model.module, <span class="string">&quot;saved_model.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="其他注意事项"><span class="post-title-index">3.6. </span><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h3><p><strong>验证集如何处理</strong></p>
<p>验证集一般在一个 epoch 的训练之后，可以使用多卡，也可以使用单卡。</p>
<ul>
<li>使用多卡时注意也要使用 <code>DistributedSampler</code></li>
<li>使用单卡时只在一个进程上操作，注意此时要用 <code>model.module(inputs)</code> 而不是 <code>model(inputs)</code> 来前向传播，并且其他进程通过 <code>torch.distributed.barrier()</code> 来等待主进程完成 validation</li>
</ul>
<p><strong>BN 层处理</strong></p>
<p>BN 中有 moving mean 和 moving variance 这两个 buffer，这两个 buffer 的更新依赖于当前训练轮次的 batch 数据的计算结果，在普通模式下，每张卡单独计算自己的 moving mean 和 moving variance，在单卡上的 batch size 比较小的时候，这样的 BN 实际上没有起到应有个的效果。</p>
<p>实现真正的多卡 BN，DDP 中可以使用 SyncBN利用分布式通讯接口在各卡间进行通讯，从而能利用所有数据进行 BN 计算。为了尽可能地减少跨卡传输量，<strong>SyncBN 做了一个关键的优化，即只传输各自进程的各自的 <code>小batch mean</code> 和 <code>小batch variance</code>，而不是所有数据。</strong>具体流程如下：</p>
<ol>
<li><p>前向传播</p>
</li>
<li><ol>
<li><p>在各进程上计算各自的 <code>小batch mean</code> 和 <code>小batch variance</code></p>
<ol>
<li><p>各自的进程对各自的 <code>小batch mean</code> 和 <code>小batch variance</code> 进行 <code>all_gather</code> 操作，每个进程都得到全局量。</p>
</li>
<li><ul>
<li>注释：只传递 mean 和 variance，而不是整体数据，可以大大减少通讯量，提高速度。</li>
</ul>
</li>
<li><p>每个进程分别计算总体 mean 和总体 variance，得到一样的结果</p>
</li>
<li><ul>
<li>注释：在数学上是可行的，有兴趣的同学可以自己推导一下。</li>
</ul>
</li>
<li><p>接下来，延续正常的 BN 计算。</p>
</li>
<li><ul>
<li>注释：因为从前向传播的计算数据中得到的 <code>batch mean</code> 和 <code>batch variance</code> 在各卡间保持一致，所以，<code>running_mean</code> 和 <code>running_variance</code>就能保持一致，不需要显式地同步了！</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
<li><p>后向传播：和正常的一样</p>
</li>
</ol>
<p>在代码中引入 SynvBN 也比较简单，在模型包裹成 DDP 模型前加一句话就可以了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:&#123;&#125;&quot;</span>, local_rank)</span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这句代码，会将普通BN替换成SyncBN</span></span><br><span class="line">model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)</span><br><span class="line"></span><br><span class="line">model = DDP(model, device_ids=[local_rank], output_device=local_rank)</span><br></pre></td></tr></table></figure>
<p>这样的同步 BN 可能会减慢运行速度，所有有的代码并没有使用。</p>
<p>关于 BN 层的处理来自知乎文章 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250471767">DDP系列第三篇：实战与技巧</a></p>
<p><strong>学习率调整</strong></p>
<p>有的代码会把学习率给增大，就是多卡的学习率等于原来单卡的学习率乘卡的数量。</p>
<p>这一步可能不是必要，也可以在多卡训练的时候就探索设置适合多卡的学习率。</p>
<p><strong>载入权重</strong></p>
<p>载入权重的时候要注意设置 <code>map_location</code>，使得权重载入到当前这张卡上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = torch.load(ckpt_path, map_location=device)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;state_dict&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h2 id="使用-torch-multiprocessing-取代启动器"><span class="post-title-index">4. </span><a href="#使用-torch-multiprocessing-取代启动器" class="headerlink" title="使用 torch.multiprocessing 取代启动器"></a>使用 torch.multiprocessing 取代启动器</h2><p>PyTorch 把 <code>multiprocessing</code> 库封装成 <code>torch.multiprocessing</code>，可以使得单卡、DDP 下的外部调用一致，即不用使用 <code>torch.distributed.launch</code>。</p>
<p>使用时，只需要调用 <code>torch.multiprocessing.spawn</code>，<code>torch.multiprocessing</code> 就会帮助我们自动创建进程。<code>mp.spawn</code> 给定一个函数 <code>main_work</code>，开启 <code>ngpus_per_node</code> 个进程来执行 <code>main_worker</code>，<code>main_worker</code> 的参数通过 <code>args</code> 传递：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))</span><br></pre></td></tr></table></figure>
<p><code>main_worker</code> 的第一个参数表示 <code>local_rank</code>（该参数不需要在 <code>mp.spawn</code> 中指定），这个必须要设置，后面的参数是自定义的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main_worker</span>(<span class="params">index, ngpus_per_node, args</span>):</span><br><span class="line">    <span class="comment"># index 就代表 local_rank</span></span><br><span class="line">    <span class="comment"># 这里写上「使用流程」中的一系列代码</span></span><br></pre></td></tr></table></figure>
<p>注意在使用 <code>mp.spawn</code> 时，由于没有 <code>torch.distributed.launch</code> 读取的默认环境变量作为配置，我们需要手动为 <code>init_process_group</code> 指定 <code>world_size</code> 和 <code>rank</code>，这个可以参照 MoCo 的官方代码。</p>
<h2 id="参考资料"><span class="post-title-index">5. </span><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>几个写的比较好的博客：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178402798">DDP系列第一篇：入门教程</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.kamino.link/2021/12/09/Pytorch%20DDP%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9/#Pytorch-DDP%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9">Pytorch DDP使用方法以及注意点</a></li>
<li><a target="_blank" rel="noopener" href="https://tangh.github.io/articles/multi-gpu-and-mix-precision-in-pytorch/">PyTorch 中多卡及混合精度使用方法</a></li>
</ol>
<p>几个看过的视频：</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1yt4y1e7sZ?share_source=copy_web">霹雳吧啦Wz：pytorch多GPU并行训练教程</a></p>
<p> 该视频的代码：<a target="_blank" rel="noopener" href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/07dfcd7cd3/pytorch_classification/train_multi_GPU/train_multi_gpu_using_launch.py">GitHub</a></p>
</li>
<li><p>Up 主 <a target="_blank" rel="noopener" href="https://space.bilibili.com/2430945">蓝染惣右介灬</a> 系列两篇：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Fb4y1r79A?share_source=copy_web">Pytorch多卡加速训练（一）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1so4y1Q7my?share_source=copy_web">Pytorch多卡加速训练（二）代码篇</a></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1xZ4y1S7dG?share_source=copy_web">deep_thoughts：PyTorch33——多GPU分布式训练教程</a></p>
</li>
</ol>
<p>一个 GitHub 仓库的 README，主要参考了 <code>mp.spawn</code> 相关的内容：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/tczhangzhi/pytorch-distributed">tczhangzhi/pytorch-distributed</a></li>
</ol>
<p>我对 MoCo 官方代码的注释：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/TimeOverflow/moco">TimeOverflow/moco</a></li>
</ol>
<p>官方文档：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/distributed.html">torch.distributed</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/multiprocessing.html">torch.multiprocessing</a></li>
</ol>
<p><strong>后记</strong></p>
<p>今天是 2022 年 4 月 8 号，封校自 2022 年 3 月 9 号开始，到今天正好一个月。解封的希望来了又去，现在遥遥无期，疫情到来打乱了我的计划，也让我失去了毕业后的假期（这可能是今后很长一段时间我能拥有的最长、最自由的假期了）。我恨疫情，更恨上海这帮子故意搞乱的官员。希望赶紧能有个空隙让我滚出上海的，现在到外地都需要隔离，但至少让我有个能隔离的机会。祈求。</p>
<p><strong>更新记录：</strong></p>
<ul>
<li>2022-4-8：第一版上传</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Pytorch-%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/" rel="tag"># Pytorch 多卡训练</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/15/paper-byol/" rel="prev" title="论文阅读：Bootstrap Your Own Latent A New Approach to Self-Supervised Learning">
      <i class="fa fa-chevron-left"></i> 论文阅读：Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-text">1. 原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">2. 基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="nav-text">3. 使用流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="nav-text">3.1. 准备工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%85%E8%A3%B9%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.2. 包裹模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-text">3.3. 数据并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-text">3.4. 训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.5. 保存模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-text">3.6. 其他注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-torch-multiprocessing-%E5%8F%96%E4%BB%A3%E5%90%AF%E5%8A%A8%E5%99%A8"><span class="nav-text">4. 使用 torch.multiprocessing 取代启动器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-text">5. 参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="TimeOverflow"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">TimeOverflow</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/TimeOverflow" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;TimeOverflow" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhangz1220@outlook.com" title="E-Mail → mailto:zhangz1220@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TimeOverflow</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">123k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:52</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'N3GrQmFE8RVjH8Sk6s4B7xnY-9Nh9j0Va',
      appKey     : '4KwTSki3hoVvxnEwsxogPOKb',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
